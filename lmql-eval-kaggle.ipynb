{"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":8138088,"sourceType":"datasetVersion","datasetId":4811104},{"sourceId":8142763,"sourceType":"datasetVersion","datasetId":4814554}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# from google.colab import drive\n# drive.mount('/content/drive')","metadata":{"id":"zu57G7nJr2vd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install -q -U lmql[hf]\n!pip install -q -U bitsandbytes\n!pip install -q -U git+https://github.com/huggingface/transformers.git\n!pip install -q -U git+https://github.com/huggingface/peft.git\n!pip install -q -U git+https://github.com/huggingface/accelerate.git","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-kzSyjEz62iX","outputId":"2f274fc9-79b2-493d-d288-440cee2b0a90","execution":{"iopub.status.busy":"2024-04-17T05:59:17.398684Z","iopub.execute_input":"2024-04-17T05:59:17.399485Z","iopub.status.idle":"2024-04-17T06:01:31.173002Z","shell.execute_reply.started":"2024-04-17T05:59:17.399457Z","shell.execute_reply":"2024-04-17T06:01:31.171580Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"import lmql\nimport json\nimport numpy as np\nimport torch\n# import torch\n# from transformers import BitsAndBytesConfig\n\n# quantization_config = BitsAndBytesConfig(\n#    load_in_4bit=True,\n#    bnb_4bit_quant_type=\"nf4\",\n#    bnb_4bit_use_double_quant=True,\n#    bnb_4bit_compute_dtype=torch.bfloat16\n# )\n\nmodel = lmql.model(\"local:mistralai/Mistral-7B-Instruct-v0.2\", load_in_4bit=True, device_map=\"auto\")\n# model = lmql.model(\"local:microsoft/phi-2\", device_map=\"auto\", load_in_4bit=True)","metadata":{"id":"-M_fntcd6777","execution":{"iopub.status.busy":"2024-04-17T07:48:43.129112Z","iopub.execute_input":"2024-04-17T07:48:43.129934Z","iopub.status.idle":"2024-04-17T07:48:43.135171Z","shell.execute_reply.started":"2024-04-17T07:48:43.129902Z","shell.execute_reply":"2024-04-17T07:48:43.134112Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"code","source":"@lmql.query\nasync def score(instruction, target_context, statements):\n  \"\"\"\n  argmax\n    l = []\n    for i in range(len(statements)):\n      \"[VERDICT: verdict(instruction, target_context, statements[i])]\"\n      l.append(VERDICT)\n    return l\n  from\n    model\n  \"\"\"\n\n@lmql.query\nasync def verdict(instruction, target_context, statement):\n  \"\"\"lmql\n    \"{instruction}\\n\"\n    \"Context: {target_context}\\n\"\n    \"Statement: {statement}\\n\"\n    \"Explanation: [EXPLANATION]\" where len(TOKENS(EXPLANATION)) < 100 or STOPS_BEFORE(EXPLANATION, \"\\n\") or STOPS_BEFORE(EXPLANATION, \"\\n\\nVerdict: \")\n    e = EXPLANATION.strip()\n    \"Verdict: [VERDICT]\" where VERDICT in set([\"True\", \"False\"])\n    return [VERDICT, e]\n  \"\"\"\n\nfaithfulness_instruction = \"You are given a pair of context and statement. You want to judge whether the statement can be inferred from the context provided. You should provide explanation for reasoning before provoding verdict. Your explanation should be concise.\"\nanswer_relevancy_instruction = \"You are given an answer for a question. You want to generate some questions for the given answer to guess the original answer.\"\ncontext_relevancy_instruction = \"You are given a pair of context and statement. You want to judge whether a statement is relevant to the context provided. You should provide explanation for reasoning before provoding verdict. Your explanation should be concise.\"\n\ncontext_precision_instruction = \"You are given a pair of context and statement. You want to judge whether the statement is relevant to arrive at the context. You should provide explanation for reasoning before provoding verdict. Your explanation should be concise.\"\ncontext_recall_instruction = \"You are given a pair of context and statement. You want to judge whether a statement can be inferred from the context provided. You should provide explanation for reasoning before provoding verdict. Your explanation should be concise.\"","metadata":{"id":"8IMBMXTqp66z","execution":{"iopub.status.busy":"2024-04-17T08:08:00.630634Z","iopub.execute_input":"2024-04-17T08:08:00.631515Z","iopub.status.idle":"2024-04-17T08:08:00.972174Z","shell.execute_reply.started":"2024-04-17T08:08:00.631482Z","shell.execute_reply":"2024-04-17T08:08:00.971150Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"code","source":"import json\n# with open(\"/kaggle/input/zero-shot-eval/zero_shot_eval_dataset_with_statements.json\", \"r\") as f:\nwith open(\"/kaggle/input/few-shot-eval/few_shot_eval_dataset_with_statements.json\", \"r\") as f:\n# with open(\"/content/drive/MyDrive/Colab Notebooks/FYP_Eval/Dataset/zero_shot_eval_dataset_with_statements.json\", \"r\") as f:\n# with open(\"/content/drive/MyDrive/Colab Notebooks/FYP_Eval/Dataset/few_shot_eval_dataset_with_statements.json\", \"r\") as f:\n    data = json.load(f)\n    # selecting the first 30 rows for valid questions only, skipped invalid questions\n    data = data[:30]","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5osyVqhGE0Pn","outputId":"62390174-7c2d-4b2c-d970-646f3e2dae1b","execution":{"iopub.status.busy":"2024-04-17T06:39:29.630033Z","iopub.execute_input":"2024-04-17T06:39:29.630964Z","iopub.status.idle":"2024-04-17T06:39:29.649975Z","shell.execute_reply.started":"2024-04-17T06:39:29.630928Z","shell.execute_reply":"2024-04-17T06:39:29.649022Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"# calculating faithfulness, requiring statements of actual output and context\nverdict_pairs_list = []\nfor i in range(len(data)):\n    print(f\"evaluating {i+1}th row\")\n    row = data[i]\n    # we want to know the proportion of output statements that can be inferred from context\n    context = \" \".join(row[\"retrieval_context\"])\n    statements = row[\"actual_output_statements\"]\n    verdict_pairs = await score(faithfulness_instruction, context, statements)\n    verdict_pairs_list.append(verdict_pairs)\n\nfaithfulness_verdicts = []\n# for all verdict results, we want to create a dict / json to store and map each verdict to its explanation, statement and context\nfor i, row in enumerate(data):\n    context = \" \".join(row[\"retrieval_context\"])\n    statements = row[\"actual_output_statements\"]\n    verdict_dicts = []\n    for j, statement in enumerate(statements):\n        verdict, explanation = verdict_pairs_list[i][j]\n        verdict_dict = {\n            \"Context\": context,\n            \"Statement\": statement,\n            \"Verdict\": verdict,\n            \"Explanation\": explanation\n        }\n        verdict_dicts.append(verdict_dict)\n    faithfulness_verdicts.append(verdict_dicts)\n\nfaithfulness_scores = []\n# each verdict_dicts represent a row in original data\nfor verdict_dicts in faithfulness_verdicts:\n    count_True = 0\n    count_verdict = 0\n    for  verdict_dict in verdict_dicts:\n        count_verdict += 1\n        if verdict_dict[\"Verdict\"] == \"True\":\n            count_True += 1\n    # faithfulness is proportion of relevant statements to context\n    f = count_True / count_verdict\n    faithfulness_scores.append(f)\nprint(np.mean(faithfulness_scores))\n\nfaithfulness = {\n    \"Scores\": faithfulness_scores,\n    \"Verdicts\": faithfulness_verdicts\n}\n\n# evaluation time ~20mins, average score = 0.8022\nwith open(\"./faithfulness.json\", \"w\") as f:\n  json.dump(faithfulness, f)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7ugzTCR9FrQB","outputId":"60a4921d-dfee-4ace-96b9-6270c89f4308","execution":{"iopub.status.busy":"2024-04-17T06:39:33.710477Z","iopub.execute_input":"2024-04-17T06:39:33.710914Z","iopub.status.idle":"2024-04-17T06:58:25.590959Z","shell.execute_reply.started":"2024-04-17T06:39:33.710878Z","shell.execute_reply":"2024-04-17T06:58:25.590013Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stdout","text":"evaluating 1th row\nevaluating 2th row\nevaluating 3th row\nevaluating 4th row\nevaluating 5th row\nevaluating 6th row\nevaluating 7th row\nevaluating 8th row\nevaluating 9th row\nevaluating 10th row\nevaluating 11th row\nevaluating 12th row\nevaluating 13th row\nevaluating 14th row\nevaluating 15th row\nevaluating 16th row\nevaluating 17th row\nevaluating 18th row\nevaluating 19th row\nevaluating 20th row\nevaluating 21th row\nevaluating 22th row\nevaluating 23th row\nevaluating 24th row\nevaluating 25th row\nevaluating 26th row\nevaluating 27th row\nevaluating 28th row\nevaluating 29th row\nevaluating 30th row\n0.8627777777777779\n","output_type":"stream"}]},{"cell_type":"code","source":"# calculating context relevancy, requiring context and question\nverdict_pairs_list = []\nfor i in range(len(data)):\n    row = data[i]\n    # we want to know proportion of context statements that are relevant to user query\n    context = row[\"input\"]\n    statements = row[\"context_statements\"]\n    print(f\"evaluating {i+1}th row with {len(statements)} statements...\") # it is quite long waiting time\n    verdict_pairs = await score(context_relevancy_instruction, context, statements)\n    verdict_pairs_list.append(verdict_pairs)\n\ncontext_relevancy_verdicts = []\n# for all verdict results, we want to create a dict / json to store and map each verdict to its explanation, statement and context\nfor i, row in enumerate(data):\n    context = row[\"input\"]\n    statements = row[\"context_statements\"]\n    verdict_dicts = []\n    for j, statement in enumerate(statements):\n        verdict, explanation = verdict_pairs_list[i][j]\n        verdict_dict = {\n            \"Context\": context,\n            \"Statement\": statement,\n            \"Verdict\": verdict,\n            \"Explanation\": explanation\n        }\n        verdict_dicts.append(verdict_dict)\n    context_relevancy_verdicts.append(verdict_dicts)\n\ncontext_relevancy_scores = []\n# each verdict_dicts represent a row in original data\nfor verdict_dicts in context_relevancy_verdicts:\n    count_True = 0\n    count_verdict = 0\n    for  verdict_dict in verdict_dicts:\n        count_verdict += 1\n        if verdict_dict[\"Verdict\"] == \"True\":\n            count_True += 1\n    # context_relevancy is proportion of relevant statements to context\n    cr = count_True / count_verdict\n    context_relevancy_scores.append(cr)\nprint(np.mean(context_relevancy_scores))\n\ncontext_relevancy = {\n    \"Scores\": context_relevancy_scores,\n    \"Verdicts\": context_relevancy_verdicts\n}\n\nwith open(\"./context_relevancy.json\", \"w\") as f:\n    json.dump(context_relevancy, f)","metadata":{"execution":{"iopub.status.busy":"2024-04-17T07:01:51.357269Z","iopub.execute_input":"2024-04-17T07:01:51.357630Z","iopub.status.idle":"2024-04-17T07:44:38.291657Z","shell.execute_reply.started":"2024-04-17T07:01:51.357602Z","shell.execute_reply":"2024-04-17T07:44:38.290460Z"},"trusted":true},"execution_count":21,"outputs":[{"name":"stdout","text":"evaluating 1th row with 6 statements...\nevaluating 2th row with 5 statements...\nevaluating 3th row with 7 statements...\nevaluating 4th row with 7 statements...\nevaluating 5th row with 3 statements...\nevaluating 6th row with 14 statements...\nevaluating 7th row with 14 statements...\nevaluating 8th row with 14 statements...\nevaluating 9th row with 14 statements...\nevaluating 10th row with 13 statements...\nevaluating 11th row with 5 statements...\nevaluating 12th row with 4 statements...\nevaluating 13th row with 5 statements...\nevaluating 14th row with 5 statements...\nevaluating 15th row with 6 statements...\nevaluating 16th row with 4 statements...\nevaluating 17th row with 6 statements...\nevaluating 18th row with 11 statements...\nevaluating 19th row with 5 statements...\nevaluating 20th row with 6 statements...\nevaluating 21th row with 8 statements...\nevaluating 22th row with 3 statements...\nevaluating 23th row with 5 statements...\nevaluating 24th row with 13 statements...\nevaluating 25th row with 5 statements...\nevaluating 26th row with 6 statements...\nevaluating 27th row with 6 statements...\nevaluating 28th row with 6 statements...\nevaluating 29th row with 2 statements...\nevaluating 30th row with 7 statements...\n0.1486291486291486\n","output_type":"stream"}]},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModel\n\n# Load model from HuggingFace Hub\ntokenizer = AutoTokenizer.from_pretrained('BAAI/bge-large-en-v1.5')\nembed_model = AutoModel.from_pretrained('BAAI/bge-large-en-v1.5')\nembed_model.eval()\n\n# load actual question and generated questions from dataset\ngenerated_questions = [row[\"generated_questions\"] for row in data]\nactual_questions = [row[\"input\"] for row in data]\n\nanswer_relevancy_verdicts = []\nanswer_relevancy_scores = []\n# calculate answer relevancy, requiring user queries and generated questions\nfor i in range(len(data)):\n    print(f\"evaluating {i+1}th row\")\n    encoded_generated_questions = tokenizer(generated_questions[i], padding=True, truncation=True, return_tensors='pt', max_length=512)\n    encoded_actual_question = tokenizer([actual_questions[i]], padding=True, truncation=True, return_tensors='pt', max_length=512)\n    with torch.no_grad():\n        model_output = embed_model(**encoded_generated_questions) \n        embeddings_generated_questions = model_output[0][:, 0]\n        model_output = embed_model(**encoded_actual_question)\n        embeddings_actual_question = model_output[0][:, 0]\n    # normalize embeddings\n    embeddings_generated_questions = torch.nn.functional.normalize(embeddings_generated_questions, p=2, dim=1)\n    embeddings_actual_question = torch.nn.functional.normalize(embeddings_actual_question, p=2, dim=1)\n    \n    # Compute cosine similarity\n    embeddings_actual_question = embeddings_actual_question.repeat(embeddings_generated_questions.size(0), 1) # to make both tensors have same dimension\n    cos = torch.nn.CosineSimilarity(dim=1)\n    cosine_similarity = cos(embeddings_generated_questions, embeddings_actual_question)\n    \n    verdict_dicts = []\n    # Store result in dict/json\n    for j in range(len(generated_questions[i])):\n        verdict_dict = {\n            \"Actaul_question\": actual_questions[i],\n            \"Generated_questions\": generated_questions[i][j],\n            \"Scores\": float(cosine_similarity[j]),\n        }\n        verdict_dicts.append(verdict_dict)\n    answer_relevancy_verdicts.append(verdict_dicts)\n    answer_relevancy_scores.append(torch.mean(cosine_similarity).item())\nprint(np.mean(answer_relevancy_scores))\n\nanswer_relevancy = {\n    \"Scores\": answer_relevancy_scores,\n    \"Verdicts\": answer_relevancy_verdicts\n}\n\nwith open(\"./answer_relevancy.json\", \"w\") as f:\n    json.dump(answer_relevancy, f)","metadata":{"execution":{"iopub.status.busy":"2024-04-17T07:47:17.672759Z","iopub.execute_input":"2024-04-17T07:47:17.673465Z","iopub.status.idle":"2024-04-17T07:47:37.234377Z","shell.execute_reply.started":"2024-04-17T07:47:17.673435Z","shell.execute_reply":"2024-04-17T07:47:37.233429Z"},"trusted":true},"execution_count":28,"outputs":[{"name":"stdout","text":"evaluating 1th row\nevaluating 2th row\nevaluating 3th row\nevaluating 4th row\nevaluating 5th row\nevaluating 6th row\nevaluating 7th row\nevaluating 8th row\nevaluating 9th row\nevaluating 10th row\nevaluating 11th row\nevaluating 12th row\nevaluating 13th row\nevaluating 14th row\nevaluating 15th row\nevaluating 16th row\nevaluating 17th row\nevaluating 18th row\nevaluating 19th row\nevaluating 20th row\nevaluating 21th row\nevaluating 22th row\nevaluating 23th row\nevaluating 24th row\nevaluating 25th row\nevaluating 26th row\nevaluating 27th row\nevaluating 28th row\nevaluating 29th row\nevaluating 30th row\n0.815654041369756\n","output_type":"stream"}]},{"cell_type":"code","source":"# calculating context recall, requiring statements of actual output and context\nverdict_pairs_list = []\nfor i in range(len(data)):\n    print(f\"evaluating {i+1}th row\")\n    row = data[i]\n    # we want to know the proportion of ground truth statements that can be inferred from context\n    context = \" \".join(row[\"retrieval_context\"])\n    statements = row[\"actual_output_statements\"]\n    verdict_pairs = await score(context_recall_instruction, context, statements)\n    verdict_pairs_list.append(verdict_pairs)\n\ncontext_recall_verdicts = []\n# for all verdict results, we want to create a dict / json to store and map each verdict to its explanation, statement and context\nfor i, row in enumerate(data):\n    context = \" \".join(row[\"retrieval_context\"])\n    statements = row[\"actual_output_statements\"]\n    verdict_dicts = []\n    for j, statement in enumerate(statements):\n        verdict, explanation = verdict_pairs_list[i][j]\n        verdict_dict = {\n            \"Context\": context,\n            \"Statement\": statement,\n            \"Verdict\": verdict,\n            \"Explanation\": explanation\n        }\n        verdict_dicts.append(verdict_dict)\n    context_recall_verdicts.append(verdict_dicts)\n\ncontext_recall_scores = []\n# each verdict_dicts represent a row in original data\nfor verdict_dicts in context_recall_verdicts:\n    count_True = 0\n    count_verdict = 0\n    for  verdict_dict in verdict_dicts:\n        count_verdict += 1\n        if verdict_dict[\"Verdict\"] == \"True\":\n            count_True += 1\n    # context_recall is proportion of relevant statements to context\n    cr = count_True / count_verdict\n    context_recall_scores.append(cr)\nprint(np.mean(context_recall_scores))\n\ncontext_recall = {\n    \"Scores\": context_recall_scores,\n    \"Verdicts\": context_recall_verdicts\n}\n\nwith open(\"./context_recall.json\", \"w\") as f:\n    json.dump(context_recall, f)","metadata":{"execution":{"iopub.status.busy":"2024-04-17T07:48:51.382922Z","iopub.execute_input":"2024-04-17T07:48:51.383282Z","iopub.status.idle":"2024-04-17T08:07:46.735467Z","shell.execute_reply.started":"2024-04-17T07:48:51.383254Z","shell.execute_reply":"2024-04-17T08:07:46.734237Z"},"trusted":true},"execution_count":35,"outputs":[{"name":"stdout","text":"evaluating 1th row\nevaluating 2th row\nevaluating 3th row\nevaluating 4th row\nevaluating 5th row\nevaluating 6th row\nevaluating 7th row\nevaluating 8th row\nevaluating 9th row\nevaluating 10th row\nevaluating 11th row\nevaluating 12th row\nevaluating 13th row\nevaluating 14th row\nevaluating 15th row\nevaluating 16th row\nevaluating 17th row\nevaluating 18th row\nevaluating 19th row\nevaluating 20th row\nevaluating 21th row\nevaluating 22th row\nevaluating 23th row\nevaluating 24th row\nevaluating 25th row\nevaluating 26th row\nevaluating 27th row\nevaluating 28th row\nevaluating 29th row\nevaluating 30th row\n0.801111111111111\n","output_type":"stream"}]},{"cell_type":"code","source":"# calculating context precision, requiring context and ground truth\nverdict_pairs_list = []\nfor i in range(len(data)):\n    print(f\"evaluating {i+1}th row\")\n    row = data[i]\n    # we want to know whether positions of relevant nodes are higher than non relevant one\n    context = row[\"expected_output\"]\n    statements = row[\"retrieval_context\"]\n    verdict_pairs = await score(context_precision_instruction, context, statements)\n    verdict_pairs_list.append(verdict_pairs)\n\ncontext_precision_verdicts = []\n# for all verdict results, we want to create a dict / json to store and map each verdict to its explanation, statement and context\nfor i, row in enumerate(data):\n    context = row[\"expected_output\"]\n    statements = row[\"retrieval_context\"]\n    verdict_dicts = []\n    for j, statement in enumerate(statements):\n        verdict, explanation = verdict_pairs_list[i][j]\n        verdict_dict = {\n            \"Context\": context,\n            \"Statement\": statement,\n            \"Verdict\": verdict,\n            \"Explanation\": explanation\n        }\n        verdict_dicts.append(verdict_dict)\n    context_precision_verdicts.append(verdict_dicts)\n\ncontext_precision_scores = []\n# each verdict_dicts represent a row in original data\nfor verdict_dicts in context_precision_verdicts:\n    count_True = 0\n    count_verdict = 0\n    for  verdict_dict in verdict_dicts:\n        count_verdict += 1\n        if verdict_dict[\"Verdict\"] == \"True\":\n            count_True += 1\n    # context_precision is proportion of relevant statements to context\n    cp = count_True / count_verdict\n    context_precision_scores.append(cp)\nprint(np.mean(context_precision_scores))\n\ncontext_precision = {\n    \"Scores\": context_precision_scores,\n    \"Verdicts\": context_precision_verdicts\n}\n\nwith open(\"./context_precision.json\", \"w\") as f:\n    json.dump(context_precision, f)","metadata":{"execution":{"iopub.status.busy":"2024-04-17T08:08:07.638208Z","iopub.execute_input":"2024-04-17T08:08:07.638912Z","iopub.status.idle":"2024-04-17T08:19:04.844558Z","shell.execute_reply.started":"2024-04-17T08:08:07.638882Z","shell.execute_reply":"2024-04-17T08:19:04.843569Z"},"trusted":true},"execution_count":37,"outputs":[{"name":"stdout","text":"evaluating 1th row\nevaluating 2th row\nevaluating 3th row\nevaluating 4th row\nevaluating 5th row\nevaluating 6th row\nevaluating 7th row\nevaluating 8th row\nevaluating 9th row\nevaluating 10th row\nevaluating 11th row\nevaluating 12th row\nevaluating 13th row\nevaluating 14th row\nevaluating 15th row\nevaluating 16th row\nevaluating 17th row\nevaluating 18th row\nevaluating 19th row\nevaluating 20th row\nevaluating 21th row\nevaluating 22th row\nevaluating 23th row\nevaluating 24th row\nevaluating 25th row\nevaluating 26th row\nevaluating 27th row\nevaluating 28th row\nevaluating 29th row\nevaluating 30th row\n0.5333333333333333\n","output_type":"stream"}]}]}